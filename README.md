**LipNet-SentenceLipreading** is an end-to-end deep learning project designed for sentence-level lipreading, which converts visual speech (lip movements) in video frames directly into text. The model integrates several advanced neural network architectures, including spatiotemporal convolutional neural networks (STCNNs), bidirectional gated recurrent units (Bi-GRUs), and Connectionist Temporal Classification (CTC) loss. STCNNs are employed to extract both spatial and temporal features from video sequences, enabling the model to recognize the dynamics of lip movements over time. Following this, Bi-GRUs are used to model the temporal dependencies in the extracted features, allowing the network to effectively process sequences of varying lengths. The CTC loss facilitates training by aligning the modelâ€™s predictions with the target text without requiring manual segmentation of the input data. This innovative approach has resulted in state-of-the-art accuracy on the GRID dataset, outperforming previous word-level lipreading methods and even human lipreaders. The project demonstrates significant potential applications in areas such as speech recognition in noisy environments, silent communication technologies, and support for individuals with hearing impairments.
